\chapter[Идентификация нелинейных стохастических систем]{%
  Идентификация нелинейных \chapternewline
  Стохастических систем
}

Как было отмечено в подразделе~\ref{ssec:state_nonlinear},
проблема идентификации нелинейных стохастических систем разработана
в недостаточной степени.
Существующие исследования нельзя считать достаточно завершенными:
отсутствует обоснование используемых критериев,
не приводится сравнительный численный анализ различных алгоритмов,
отсутствуют рекомендации по применению того или иного алгоритма.

В данной работе для решения данной задачи предлагается использовать методы,
известные в теории оценивания параметров по косвенных измерениям:
нелинейный метод наименьших квадратов~\cite{mukha_2009} и
метод рядов Тейлора~\cite{mukha_2000}.

Данная глава посвящена выявлению условий предпочтительного
использования МНК и МРТ для идентификации нелинейных
стохастических систем второго типа.

\section{Методы и алгоритмы идентификации}

\subsection{Нелинейный метод наименьших квадратов}\label{ssec:nonlinear_method_lse}

Один из подходов к оцениванию параметров системы~\eqref{eq:model_general}
состоит в следующем.
Можно <<закрыть глаза>> на существование ошибок измерений
входной переменной, то есть считать, что \( \varepsilon_x = 0 \),
и вместо данной модели рассматривать модель
\begin{equation*}
  \overline{y} = \psi(\overline{\theta}, \overline{x}) + \overline{\varepsilon}_y,
\end{equation*}
где \( \overline{x}, \overline{y} \)
"--- векторы наблюдаемых значений входной и выходной переменной, \\
\hspace*{7mm} \( \overline{\theta} \)
"--- вектор фактических значений параметров, \\
\hspace*{7mm} \( \psi \)
"--- векторная функция регрессии, \\
\hspace*{7mm} \( \overline{\varepsilon_y} \)
"--- вектор независимых ошибок наблюдений выхода,
распределенных по нормальному закону \( N(0, R_y) \).

Тогда оценка вектора параметров объекта определяется выражением
\begin{equation}
  \label{eq:nonlinear_lse_estimate}
  \hat{\overline{\theta}}_{\text{МНК}} =
  \overline{\theta}_0 + (Q^T R^{-1}_{y} Q)^{-1} Q^T R^{-1}_{y} (y - \psi(\overline{\theta}_0, x)),
\end{equation}
где \( \overline{\theta}_0 \) --- опорная точка,
\( Q = \dfrac{\partial \psi(\overline{\theta}_0, \overline{x}) }{ \partial \overline{\theta}_0 } \).

В качестве опорной точки \( \overline{\theta}_0 \) можно использовать значения
\( \theta_1, \dotsc, \theta_m \),
полученные в результате численного решения системы уравнений
\begin{equation}
  \label{eq:nonlinear_basic}
  \overline{y} - \psi( \overline{\theta}, \overline{x} ) = 0,
\end{equation}
где \( \overline{x} = (x_1, \ldots, x_m), \overline{y} = (y_1, \ldots, y_m) \)
"--- наблюдения входа и выхода системы, называемые в данной работе опорными.

В качестве опорных наблюдений могут выступать, например:
\begin{itemize}
\item первые \( m \) элементов множества пар наблюдений,
  упорядоченного по возрастанию измеренных значений входа:
  \[
    (x_1, y_1), (x_2, y_2), \dotsc , (x_m, y_m), \:
    x_i < x_{i+1}, \;
    i = \overline{1, n};
  \]
\item \( m \) равноотстоящих друг от друга элементов множества пар наблюдений,
  упорядоченного по возрастанию измеренных значений входа:
  \[
    (x_{k}, y_{k}), (x_{2k}, y_{2k}) , \dotsc , (x_{mk}, y_{mk}), \:
    k = \lfloor \dfrac{n}{m} \rfloor, \:
    x_i < x_{i+1}, \;
    i = \overline{1, n};
  \]
\item \( m \) средних значений подмножеств множества пар наблюдений,
  упорядоченного по возрастанию измеренных значений входа:
  \begin{equation}
    \begin{gathered}
      ( \overline{x_{k}}, \overline{y_{k}} ),
      ( \overline{x_{2k}}, \overline{y_{2k}} ),
      \dotsc ,
      ( \overline{x_{mk}}, \overline{y_{mk}}), \\
      \overline{x_{ik}} = \dfrac{1}{m} \sum_{j = 1+(i-1)k}^{ik} x_j, \:
      \overline{y_{ik}} = \dfrac{1}{m} \sum_{j = 1+(i-1)k}^{ik} y_j, \\
      k = \lfloor \dfrac{n}{m} \rfloor \:
      x_i < x_{i+1}, \; i = \overline{1, n}.
    \end{gathered}
    \label{eq:nonlinear_base_values}
  \end{equation}
\end{itemize}

Для уточнения оценки, полученной по формуле~\eqref{eq:nonlinear_lse_estimate}, можно
организовать итерационную процедуру, заменяя опорную точку полученной оценкой.

\subsection{Метод рядов Тейлора}\label{ssec:nonlinear_method_mrt}

Применение метода рядов Тейлора требует иной формулировки задачи,
допускаемой формулировкой~\eqref{eq:model_general}.
Следует предположить, что \( j \)-е наблюдение вектора параметров \( \overline{\theta}_j \)
определяется как векторная функция показаний приборов:
\begin{equation}
  \label{eq:nonlinear_mrt_phi}
  \overline{\theta}_j = \phi( \overline{z}_{j} ), \: j = \overline{1, n},
\end{equation}
где вектор
\( \overline{z}^{\text{T}}_{j} =
( \overline{x}^{\text{T}}_{j}, \overline{y}^{\text{T}}_{j}) \)
имеет нормальное распределение \( N(\overline{a}_{z,j}, R_{z,j}) \)
и оцениваемый векторный параметр \( \overline{\theta} \) определяется как
\( \overline{\theta} = \phi(\overline{a}_{z,j}), \forall i = \overline{1, n} \).
В этом случае МРТ-оценка \( \hat{\overline{\theta}}_{\text{МРТ}} \) векторного параметра \( \overline{\theta} \)
определяется выражением
\begin{equation*}
  \hat{\overline{\theta}}_{\text{МРТ}} =
  \Bigg( \sum^{n}_{i=1} R^{-1}_{\theta,i} \Bigg)^{-1}
  \sum^{n}_{j=1} R^{-1}_{\theta,j} \overline{\theta}_j,
\end{equation*}
где
\( R_{\theta,i} = G_i R_{z,i} G^T_i \),
\( G_i =
\dfrac{\partial \phi( \overline{z}_{i} ) }{ \partial \overline{z}_{i} } \).

Численное значение наблюдения \( \theta_j \)~\eqref{eq:nonlinear_mrt_phi} может определяться как
решение системы уравнений~\eqref{eq:nonlinear_basic}.

\subsection{Программные реализации алгоритмов}

Были разработаны программные реализации алгоритмов, описанных в
подразделах~\ref{ssec:nonlinear_method_lse},~\ref{ssec:nonlinear_method_mrt},
представляющие собой функции языка программирования Python.
Они используют модуль для работы с многомерными данными NumPy,
а также модуль для работы с символическими выражениями SymPy.

На рисунках~\ref{lst:nonlinear_api_lse}, \ref{lst:nonlinear_api_mrt}
представлены примеры использования программных реализаций
нелинейного метода наименьших квадратов и метода рядов Тейлора для
нахождения оценок параметров \( \hat{\alpha}, \hat{\beta} \)
функции регрессии \( y = \alpha + \beta \sin{x} \).

\pagebreak
\lstinputlisting[
caption={Использование программной реализации нелинейного метода наименьших квадратов},
language={Python},
label=lst:nonlinear_api_lse,
]{lst/nonlinear_api_lse.py}

\vfill
\lstinputlisting[
caption={Использование программной реализации \\ метода рядов Тейлора},
language={Python},
label=lst:nonlinear_api_mrt,
]{lst/nonlinear_api_mrt.py}

Здесь переменные \texttt{sym\_expression} и \texttt{sym\_expression\_delta}
обозначают функцию регрессии, записанную в различных формах,
\texttt{sym\_alpha} и \texttt{sym\_beta} соответствуют оцениваемым параметрам,
а \texttt{sym\_x} и \texttt{sym\_y} "--- входу и выходу системы.
Переменной \texttt{measured\_vals\_x} обозначены измеренные значения входа системы,
а \texttt{measured\_vals\_y} соответствует значениям наблюдений выхода.
Расчет оценок параметров \texttt{lse\_alpha} и \texttt{lse\_beta} производится по
результатам двух итераций метода, в качестве опорных оценок используются нулевые значения.
Расчет оценок параметров \texttt{mrt\_alpha} и \texttt{mrt\_beta} производится
при с.~к.~о. ошибок входных и выходных измерений
\( \sigma_{\varepsilon_x } = 1, \sigma_{\varepsilon_y } = 1 \).

Исходный текст программных реализаций приведен в приложении Б.

\section{Математическая модель идентифицируемой системы}

Для обеспечения наглядности сравнения будем рассматривать случай
идентификации скалярной системы:
\begin{equation}
  \label{eq:nonlinear_model_scalar}
  \begin{aligned}
    h &= \psi(\overline{\theta}, \xi), \\
    x &= \xi + \varepsilon_x, \\
    y &= h + \varepsilon_y,
  \end{aligned}
\end{equation}
где \( \xi, h \) "--- фактические значения входной и выходной переменной, \\
\hspace*{7mm} \( \psi \) "--- скалярно-векторная функция регрессии, \\
\hspace*{7mm} \( \overline{\theta} = (\theta_1, \theta_2, \dotsc, \theta_m) \)
"--- вектор фактических значений параметров объекта, \\
\hspace*{7mm} \( x, y \) "--- наблюдаемые значения входной и выходной переменной, \\
\hspace*{6mm} \( \varepsilon_x, \varepsilon_y \)
"--- независимые ошибки измерений значений входной и выходной переменной,
распределенные по нормальному закону:
\(
\varepsilon_x = N(0, \sigma_{\varepsilon_x}),
\varepsilon_y = N(0, \sigma_{\varepsilon_y})
\).

Данная модель была использована для генерации наблюдений входа и выхода системы,
на основании которых были получены оценки её параметров
методом наименьших квадратов и методом рядов Тейлора.
Значения \( \xi_i \) выбирались из равномерного в \( [0, 10] \) распределения.
Для получения каждой оценки \( \hat{\overline{\theta}} \) использовались результаты
ста наблюдений \( ( x_i, y_i ), i = \overline{1, n}, n = 100 \).

\section{Численный анализ точности оценивания параметров}

\subsection{Методика сравнения}\label{subsec:nonlinear_comparison_conditions}

Было выполнено сравнение точности оценивания параметров \( \overline{\theta} \)
модели~\eqref{eq:nonlinear_model_scalar}
нелинейным методом наименьших квадратов и методом рядов Тейлора
с функциями регрессии различного вида,
в зависимости от с.~к.~о ошибок измеряемых значений
\( \sigma_{\varepsilon_x}, \sigma_{\varepsilon_y} \).

В качестве величины, характеризующей сравнительную точность оценивания параметров,
использовалась разность средних Евклидовых расстояний
между точными значениями параметров модели и их оценками, полученными
с помощью МНК и МРТ:
\begin{equation*}
  \begin{aligned}
    d &= d_{\text{МНК}} - d_{\text{МРТ}}, \\
    d_{\text{МНК}} &=
    \dfrac{1}{k} \sum_{j=1}^k
    \sqrt{\sum_{\text{i=1}}^m (\hat{\theta}_{\text{МНК}_{ij}} - \theta_{ij})^2}, \\
    d_{\text{МРТ}} &=
    \dfrac{1}{k} \sum_{j=1}^k
    \sqrt{\sum_{\text{i=1}}^m (\hat{\theta}_{\text{МРТ}_{ij}} - \theta_{ij})^2},
  \end{aligned}
\end{equation*}
где \( k \) "--- число оценок.

Таким образом, при \( d > 0 \) точность оценивания параметров модели с помощью МРТ
превосходит точность МНК, а при \( d < 0 \) МНК-оценки являются более точными.
При \( d = 0 \) оба метода дают оценки одинаковой точности.

Расчеты величины \( d \) производились в узлах сетки значений
\( \sigma_{\varepsilon_x}, \sigma_{\varepsilon_y} \) в прямоугольнике
\( [0, 2] \times [0, 2] \) с шагом 0{,}1.
В каждом узле сетки вычислялось \( k = 100 \) оценок.
Для расчета оценок использовались опорные значения,
рассчитанные по алгоритму~\eqref{eq:nonlinear_base_values}.
В качестве значений МНК-оценки \( \theta_{\text{МНК}} \)
были использованы значения, полученные на первой итерации метода.

\pagebreak
\subsection{Линейная функция регрессии}

Простейшим частным случаем модели~\eqref{eq:nonlinear_model_scalar} является линейная,
то есть такая, функция регрессии которой имеет вид
\[ \psi = \theta_0 + \theta_1 \xi. \]

На рисунке~\ref{fig:comparison_nonlinear_linear}
представлены линии равного уровня зависимости \( d(\sigma_{\varepsilon_x}, \sigma_{\varepsilon_y}) \)
при \( \theta_1 = 1 \).
Видно, что точность оценок параметров модели, полученных с помощью МНК,
превосходит точность МРТ-оценок.

По результатам исследования зависимости \( d(\sigma_{\varepsilon_x}, \sigma_{\varepsilon_y}) \) при
различных значениях параметров модели можно сделать следующие выводы:
\begin{enumerate}
\item Точность оценивания параметров линейной модели не зависит
  от фактического значения постоянной составляющей \( \theta_0 \).
\item МНК оценивает параметры модели в среднем более точно, чем МРТ,
  при любых значениях коэффициента усиления \( \theta_1 \).
  Данная тенденция проявляется сильнее с ростом абсолютного значения
  коэффициента усиления \( \theta_1 \).
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=135mm]{fig/nonlinear/linear/a-0_b-1.png}
  \caption{
    Сравнительная точность оценивания \\
    параметров линейной модели при \( \theta_1 = 1 \)
  }\label{fig:comparison_nonlinear_linear}
\end{figure}

\subsection{Параболическая функция регрессии}

Параболическая функция регрессии имеет следующий вид:
\[ \psi = \theta_0 + \theta_1 \xi + \theta_2 \xi^2. \]

На рисунках~\ref{fig:comparison_nonlinear_quadratic_b-0_c-1}--\ref{fig:comparison_nonlinear_quadratic_c-1} представлены линии равного уровня зависимости
\( d(\sigma_{\varepsilon_x}, \sigma_{\varepsilon_y}) \)
для параболической функции регрессии при различных значениях её параметров.
По данным рисункам можно сделать следующие выводы:
\begin{enumerate}
\item Если функция регрессии на промежутке аппроксимации является монотонно
  возрастающей или монотонно убывающей,
  то точность оценивания её параметров зависит главным образом от
  абсолютного значения её параметра \( \theta_2 \)
  (рисунок~\ref{fig:comparison_nonlinear_quadratic_b-5_c-1}).
  Чем большее абсолютное значение имеет данный параметр,
  тем более точными являются оценки, полученные МНК,
  по сравнению с МРТ-оценками
  (рисунки~\ref{fig:comparison_nonlinear_quadratic_b-0_c-1},~\ref{fig:comparison_nonlinear_quadratic_b-0},~\ref{fig:comparison_nonlinear_quadratic_b-5_c-1}).
\item В случае, когда функция регрессии на промежутке аппроксимации не является
  монотонно возрастающей или убывающей,
  метод рядов Тейлора оценивает параметры модели более точно, чем МНК
  (рисунок~\ref{fig:comparison_nonlinear_quadratic_b--5_c-1}).
\end{enumerate}

\begin{figure}[b]
  \centering
  \includegraphics[width=135mm]{fig/nonlinear/quadratic/a-0_b-0_c-1.png}
  \caption{
    Сравнительная точность оценивания \\
    параметров параболической модели при \( \theta_1 = 0, \theta_2 = 1 \)
  }\label{fig:comparison_nonlinear_quadratic_b-0_c-1}
\end{figure}

\begin{figure}[p]
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=135mm]{fig/nonlinear/quadratic/a-0_b--5_c-1.png}
    \caption{\( \theta_1 = -5 \)}\label{fig:comparison_nonlinear_quadratic_b--5_c-1}
  \end{subfigure}

  \vspace{2\baselineskip}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=135mm]{fig/nonlinear/quadratic/a-0_b-5_c-1.png}
    \caption{\( \theta_1 = 5 \)}\label{fig:comparison_nonlinear_quadratic_b-5_c-1}
  \end{subfigure}

  \vspace{\baselineskip}
  \caption{
    Сравнительная точность оценивания \\
    параметров параболической модели при \( \theta_2 = 1 \)
  }\label{fig:comparison_nonlinear_quadratic_c-1}
\end{figure}

\begin{figure}[p]
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=135mm]{fig/nonlinear/quadratic/a-0_b-0_c--5.png}
    \caption{\( \theta_2 = -5 \)}
  \end{subfigure}

  \vspace{2\baselineskip}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=135mm]{fig/nonlinear/quadratic/a-0_b-0_c-5.png}
    \caption{\( \theta_2 = 5 \)}
  \end{subfigure}

  \vspace{\baselineskip}
  \caption{
    Сравнительная точность оценивания \\
    параметров параболической модели при \( \theta_1 = 0 \)
  }\label{fig:comparison_nonlinear_quadratic_b-0}
\end{figure}

\subsection{Синусоидальная функция регрессии}

Рассматривалось семейство функций регрессии вида
\[ \psi_{\varphi} = \theta_0 + \theta_1 \sin{(\varphi \xi)}, \:
  \varphi \in \{ 0{,}2, 1, 5 \}. \]

В таблице~\ref{tbl:comparison_nonlinear_sinusoidal} приведены средние значения
величин \( d \) при различных значениях параметров \( \theta_1 \).
По данным таблицы можно сделать вывод, что в среднем МНК оценивает параметры
синусоидальных функций регрессии точнее, чем МРТ.
Сравнительная точность оценивания зависит как от частоты \( \varphi \)
(нелинейным образом), так и от коэффициента усиления
(с ростом \( \theta_1 \) точность МНК-оценивания снижается медленнее, чем МРТ).

На рисунке~\ref{fig:comparison_nonlinear_sinusoidal}
представлены линии равного уровня функции \( d(\sigma_{\varepsilon_x}, \sigma_{\varepsilon_y}) \)
для синусоидальной функции регрессии \( \psi_{1} \) при
различных значениях параметра \( \theta_1 \).
Видно, что точность оценок, полученных с помощью МНК, оказывается выше,
чем МРТ-оценок, при любых значениях с.~к.~о. ошибок измерений.
Данная тенденция проявляется наиболее сильно при
\( \sigma_{\varepsilon_x} \gg \sigma_{\varepsilon_y} \),
что связано, по-видимому, с тем, что синусоидальная функция
является периодической.

\begin{table}[h]
  \caption{%
    Средняя точность оценивания параметров синусоидальной модели в
    зависимости от частоты \( \varphi \) и фактических значений параметра \( \theta_1 \)
  }\label{tbl:comparison_nonlinear_sinusoidal}
  \begin{tabular}{| M{1.3cm} | M{1.3cm} | M{4cm} | M{4cm} | M{4cm} |}
    \hline
    \( \varphi \)
    & \( \theta_1 \)
    & \( \overline{d_{\text{МНК}}} \)
    & \( \overline{d_{\text{МРТ}}} \)
    & \( \overline{d_{\text{МНК}} - d_{\text{МРТ}}} \) \\
    \hline
    \multirow{3}{*}{0{,}5}
    & 2
    & 0{,}36
    & 0{,}45
    & -0{,}09 \\ \cline{2-5}
    & 5
    & 0{,}78
    & 1{,}03
    & -0{,}25 \\ \cline{2-5}
    & 10
    & 1{,}51
    & 2{,}03
    & -0{,}52 \\
    \hline
    \multirow{3}{*}{1}
    & 2
    & 0{,}93
    & 1{,}06
    & -0{,}12 \\ \cline{2-5}
    & 5
    & 2{,}27
    & 2{,}64
    & -0{,}37 \\ \cline{2-5}
    & 10
    & 4{,}51
    & 5{,}39
    & -0{,}87 \\
    \hline
    \multirow{3}{*}{2}
    & 2
    & 1{,}39
    & 1{,}47
    & -0{,}08 \\ \cline{2-5}
    & 5
    & 3{,}44
    & 3{,}67
    & -0{,}23 \\ \cline{2-5}
    & 10
    & 6{,}86
    & 7{,}44
    & -0{,}57 \\
    \hline
  \end{tabular}
\end{table}

\begin{figure}[p]
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=135mm]{fig/nonlinear/sinusoidal/a-0_b-2.png}
    \caption{\( \theta_1 = 2 \)}
  \end{subfigure}

  \vspace{2\baselineskip}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=135mm]{fig/nonlinear/sinusoidal/a-0_b-10.png}
    \caption{\( \theta_1 = 5 \)}
  \end{subfigure}

  \vspace{\baselineskip}
  \caption{
    Сравнительная точность оценивания \\
    параметров синусоидальной модели
  }\label{fig:comparison_nonlinear_sinusoidal}
\end{figure}

\subsection{Экспоненциальная функция регрессии}

Рассматривалась экспоненциальная функция регрессии вида
\[ \psi = e^{\theta_0 + \theta_1 \xi}, \; \theta_0 > 0, \theta_1 > 0. \]

На основании линий равного уровня зависимости \( d(\sigma_{\varepsilon_x}, \sigma_{\varepsilon_y}) \)
при различных значениях параметра \( \theta_1 \)
(рисунки~\ref{fig:comparison_nonlinear_exponential_a-2_b-0,3}--~\ref{fig:comparison_nonlinear_exponential_a-2_b-0,5}) можно сделать следующие выводы:
\begin{enumerate}
\item МРТ даёт в среднем более точные оценки параметров экспоненциальной модели, чем МНК,
  во всем диапазоне рассматриваемых значений параметров \( \theta_0, \theta_1 \).
\item Чем большее значение имеют параметры модели \( \theta_0, \theta_1 \),
  тем более точными являются оценки, полученные МРТ, по сравнению с МНК-оценками.
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=135mm]{fig/nonlinear/exponential/a-2_b-0,3.png}
  \caption{
    Сравнительная точность оценивания \\
    параметров экспоненциальной модели при \( \theta_0 = 2, \theta_1 = 0{,}3 \)
  }\label{fig:comparison_nonlinear_exponential_a-2_b-0,3}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=135mm]{fig/nonlinear/exponential/a-2_b-0,4.png}
  \caption{
    Сравнительная точность оценивания \\
    параметров экспоненциальной модели при \( \theta_0 = 2, \theta_1 = 0{,}4 \)
  }\label{fig:comparison_nonlinear_exponential_a-2_b-0,4}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=135mm]{fig/nonlinear/exponential/a-2_b-0,5.png}
  \caption{
    Сравнительная точность оценивания \\
    параметров экспоненциальной модели при \( \theta_0 = 2, \theta_1 = 0{,}5 \)
  }\label{fig:comparison_nonlinear_exponential_a-2_b-0,5}
\end{figure}

\pagebreak
\subsection{Обратная функция регрессии}

Под обратной функцией регрессии понимается выражение вида
\[ \psi = \theta_0 + \dfrac{1}{\theta_1 + \theta_2 x}. \]

Значения параметров \( \theta_1, \theta_2 \) выбирались таким образом, чтобы
их оценивание производилось по наблюдениям, соответствующим правой ветви функции \( \psi \).
В условиях моделирования, описанных в подразделе~\ref{subsec:nonlinear_comparison_conditions},
этого можно добиться, если принять
\[ c = \dfrac{\theta_1}{\theta_2} > 3 \max{(\sigma_{\varepsilon_x})} = 6. \]
Для того, чтобы исключить влияние горизонтального смещения функции регрессии,
необходимо, чтобы значение величины \( c \) сохранялось неизменным во всех экспериментах.

В таблице~\ref{tbl:comparison_nonlinear_inverse} приведены средние значения
величины \( d \) при различных значениях параметров \( \theta_1, \theta_2 \).
Нетрудно заметить, что во всех рассмотренных случаях средняя точность МНК-оценок
является неудовлетворительной.
В этих же условиях МРТ даёт оценки приемлемой точности.

На рисунке~\ref{fig:comparison_nonlinear_inverse} представлены линии равного уровня зависимости
\( d(\sigma_{\varepsilon_x}, \sigma_{\varepsilon_y}) \) при <<крайних>> значениях параметров
\( \theta_1, \theta_2 \), взятых из таблицы.
Видно, что при больших значениях ошибок с.~к.~о. наблюдений
МРТ позволяет получить значительно более точные оценки параметров, чем МНК.
Данная тенденция проявляется тем сильнее,
чем более <<крутой>> является функция регрессии при данных значениях параметров.
Кроме этого, следует отметить ряд пиковых значений на данных рисунках,
сотвествующих случаям, когда опорные оценки параметров для МНК оказались
особенно неудачными.

\begin{table}[b]
  \caption{%
    Средняя точность оценивания параметров обратной модели в
    зависимости от фактических значений параметров \( \theta_1, \theta_2 \)
  }\label{tbl:comparison_nonlinear_inverse}
  \small
  \begin{tabular}{| m{5cm} | M{3.3cm} | M{3.3cm} | M{3.5cm} |}
    \hline
    \( \theta_1, \theta_2 \)
    & \( \overline{d_{\text{МНК}}} \)
    & \( \overline{d_{\text{МРТ}}} \)
    & \( \overline{d_{\text{МНК}} - d_{\text{МРТ}}} \) \\
    \hline
    \( \theta_1 = 0{,}035, \theta_2 = 0{,}005 \)
    & 1621{,}9
    & 14{,}8
    & 1607{,}1 \\
    \hline
    \( \theta_1 = 0{,}07, \theta_2 = 0{,}01 \)
    & 2533{,}9
    & 8
    & 2525{,}9 \\
    \hline
    \( \theta_1 = 0{,}14, \theta_2 = 0{,}02 \)
    & 2189
    & 4{,}3
    & 2184{,}7 \\
    \hline
    \( \theta_1 = 0{,}28, \theta_2 = 0{,}04 \)
    & 3358{,}1
    & 2{,}3
    & 3355{,}8 \\
    \hline
  \end{tabular}
\end{table}

\begin{figure}[p]
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=135mm]{fig/nonlinear/inverse/a-0_b-0,035_c-0,005.png}
    \caption{\( \theta_1 = 0{,}035, \theta_2 = 0{,}05 \)}
  \end{subfigure}

  \vspace{2\baselineskip}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=135mm]{fig/nonlinear/inverse/a-0_b-0,28_c-0,04.png}
    \caption{\( \theta_1 = 0{,}28, \theta_2 = 0{,}4 \)}
  \end{subfigure}

  \vspace{\baselineskip}
  \caption{
    Сравнительная точность оценивания \\
    параметров обратной модели
  }\label{fig:comparison_nonlinear_inverse}
\end{figure}

\section{Выводы}

\begin{enumerate}
\item Для решения задачи идентификации нелинейных стохастических систем
  предложено использовать нелинейный метод наименьших квадратов
  и метод рядов Тейлора.
\item Разработаны программные реализации предложенных методов.
\item Выполнен численный анализ точности оценивания параметров
  нелинейных стохастических систем второго типа с помощью МНК и МРТ.
\item На основании результатов анализа описаны условия
  предпочтительного использования МНК и МРТ для
  идентификации ряда нелинейных стохастических систем второго типа.
\end{enumerate}