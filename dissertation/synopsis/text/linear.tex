\chapter[Идентификация линейных стохастических систем второго типа]{%
  Идентификация линейных стохастических систем второго типа
}

\section{Математическая модель задачи идентификации}

Математическая модель задачи идентификации линейной стохастической системы второго типа
является частным случаем общей модели~\eqref{eq:model_general}.
В этом случае скалярно-векторная функция регрессии \( \psi \) носит линейный и
детерминированный характер:
\begin{equation*}
  \psi = c_0 + \sum_j c_j \xi_j.
\end{equation*}

Величины \( \xi_j \) соответствуют наблюдению входов векторной системы,
а коэффициенты \( c_j \) являются параметрами системы, подлежащими оцениванию.

{\color{red}
Следует отметить,
что для упрощения расчетов можно принять идентифицируемую систему скалярной.
}
В этом случае модель задачи примет еще более простой вид:
\begin{equation}
  \label{eq:model_linear_scalar}
  \begin{aligned}
  h &= \alpha + \beta \xi, \\
  x &= \xi + \varepsilon_x, \\
  y &= h + \varepsilon_y,
  \end{aligned}
\end{equation}
где \( \alpha, \beta \) --- постоянная составляющая и коэффициент усиления ---
параметры идентифицируемой линейной системы.

Здесь, как и в исходной задаче, требуется,
располагая измеренными значениями входа и выхода \( x, y \),
найти оценки \( \hat{\alpha}, \hat{\beta} \) параметров системы \( \alpha, \beta \).


\section{Методы идентификации}

\subsection{Метод наименьших квадратов}

Метод наименьших квадратов ставит своей целью минимизировать сумму вертикальных расстояний
от аппроксимирующей функции (в линейном случае --- гиперплоскости) до результатов наблюдений.
Это соответствует использованию классического критерия идентификации:
\begin{equation*}
  \rho_{\text{к}} = \sum_i \rho_{\text{к}_i} \rightarrow \min.
\end{equation*}

Приведем алгоритм расчета оценок параметров линейной функции
регрессии методом наименьших квадратов~\cite{wiki_lse}.

Пусть \( y \) --- вектор-столбец наблюдений выходной переменной \( \eta \),
а \( X \) --- \( (n \times k) \)-матрица измерений вектора \( \xi \)
(строки матрицы --- векторы значений факторов в данном наблюдении,
столбцы --- векторы значений данного фактора во всех наблюдениях).
Матричное представление такой модели имеет вид:
\begin{equation*}
  y = X \theta + \varepsilon.
\end{equation*}

Вектор оценок выходной переменной и вектор остатков регрессии равны
\( \hat{y} = X \theta, \) и \( e = y - \hat{y} = y - X \theta \) соответственно.
Сумма квадратов остатков регрессии равна \( RSS = e^T e = (y - X \theta)^T (y - X \theta) \).
Дифференцируя эту функцию по вектору параметров \( \theta \) и приравняв производные к нулю,
получим систему уравнений:
\begin{equation*}
  (X^T X)b = X^T y.
\end{equation*}

Решение этой системы уравнений дает общую формулу МНК-оценок:
\begin{equation}
  \hat{\theta} = (X^{T}X)^{-1}X^{T} y.
\end{equation}

{\color{red}
  Пару слов о свойствах и применимости МНК?
  Ограничения?
}

\subsection{Метод симметричной аппроксимации}

Метод симметричной аппроксимации минимизирует сумму перпендикулярных расстояний
от аппроксимирующей гиперплоскости до результатов наблюдений.
Это соответствует использованию симметричного критерия идентификации:
\begin{equation*}
  \rho_{\text{с}} = \sum_i \rho_{\text{с}_i} \rightarrow \min.
\end{equation*}

Приведем алгоритм расчета оценок параметров линейной функции
регрессии методом симметричной аппроксимации~\cite{mukha_2016}.

Пусть \( X \) --- \( (m \times n) \)-матрица измерений.
Первые \( m - r \) строк этой матрицы представляют собой векторы значений входных
переменных в данном наблюдении,
последние \( r \) строк --- векторы значений выходных переменных в данном наблюдении,
а столбцы соответствуют векторам значений данной переменной во всех наблюдениях.

\begin{enumerate}
\item Рассчитываются выборочные моменты случайного вектора наблюдений входных и выходных переменных:
  \begin{equation*}
    \hat{A} = \dfrac{1}{n} \sum_{i=1}^n X_i, \quad
    \hat{D} = \dfrac{1}{n}  \sum_{i=1}^n (X_i - A) (X_i - A)^T.
  \end{equation*}
\item Формируется матрица \( P \), состоящая из собственных векторов матрицы \( \hat{D} \),
  расположенных в порядке убывания соответствующих им собственных чисел.
\item Формируются матрица \( C \), состоящая из \( m - r \) первых столбцов матрицы
  \( P \). Выполняется разбиение матриц \( X \), \( \hat{A} \) и \( C \) на блоки:
  \begin{equation*}
    C =
    \begin{pmatrix}
      C_{m-r} \\
      C_r
    \end{pmatrix}, \quad
    X =
    \begin{pmatrix}
      X_{m-r} \\
      X_r
    \end{pmatrix}, \quad
    \hat{A}_{\xi} =
    \begin{pmatrix}
      \hat{A}_{m-r} \\
      \hat{A}_r
    \end{pmatrix}.
  \end{equation*}
  Таким образом, матрица \( C_{m-r} \) содержит первые \( m - r \) строк матрицы \( C \),
  а \( C_{r} \) --- последние \( r \) строк матрицы \( С \).
  Вектор \( \hat{A}_{m-r} \) содержит первые \( m - r \) элементов вектора \( \hat{A} \),
  а \( \hat{A}_{r} \) --- последние \( r \) строк вектора \( \hat{A} \).
\item Рассчитываются коэффициенты линейной аппроксимирующей зависимости
  \( y = \alpha + \beta x \) по формулам:
  \begin{equation*}
    \begin{aligned}
      \alpha &= \hat{A}_r - C_r (C_{m-r})^{-1} \hat{A}_{m-r}, \\
      \beta &= C_r (C_{m-r})^{-1}.
    \end{aligned}
  \end{equation*}
\end{enumerate}

\section{Численный анализ точности методов идентификации}

Рассматривался объект вида
\begin{equation}
  \label{eq:model}
  \begin{aligned}
  h &= \psi (\alpha, \beta, \xi), \\
  x &= \xi + \varepsilon, \\
  y &= h + \delta
  \end{aligned}
\end{equation}
где
\( \xi, h \) --- фактические значения входной и выходной переменной,
\( \alpha, \beta \) --- фактические значения параметров объекта,
\( \psi \) --- скалярная функция регрессии,
\( x, y \) --- измеренные значения входной и выходной переменной,
\( \varepsilon, \delta \) --- независимые ошибки измерений значений входной и выходной переменной,
распределенные по нормальному закону:
\( \varepsilon = N(0, \sigma_{\varepsilon}), \delta = N(0, \sigma_{\delta}) \).

Исследовалась зависимость точности оценок параметров от с.~к.~о.
ошибок измерений \( \sigma_{\varepsilon}, \sigma_{\delta} \).

В качестве величины, характеризующей точность оценивания,
использовалось среднее Евклидово расстояние в пространстве параметров:
\begin{equation}
  \label{eq:quality}
  d = \frac{1}{k} \sum_{j=1}^k \sqrt{(\hat{\alpha}_j - \alpha)^2 + (\hat{\beta}_j - \beta)^2}.
\end{equation}

В работе~\cite{budny17} было выполнено сравнение точности оценок параметров
\( \hat{\alpha}, \hat{\beta} \) линейной зависимости \( \psi = \alpha + \beta \xi \),
полученных классической линейной регрессией и методом симметричной аппроксимации.

Значения \( \xi_i \) выбирались из равномерного в \( [0, 10] \) распределения.
Для получения каждой оценки \( ( \alpha, \beta ) \) использовались результаты
ста наблюдений \( ( x_i, y_i ), i = \overline{1, n}, n = 100 \).

Расчеты расстояний~\eqref{eq:quality} производились в узлах сетки значений
\( \sigma_{\varepsilon}, \sigma_{\delta} \) в прямоугольнике
\( [0, 2] \times [0, 2] \) с шагом 0{,}1.
В каждом узле сетки вычислялось \( k = 100 \) оценок.


Постановка задачи.
Точность сравнения параметров и точность предсказания.
Условия моделирования.
Графики.
Результаты (таблица)?
Выводы.