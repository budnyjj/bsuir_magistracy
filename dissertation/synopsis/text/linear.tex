\chapter[Идентификация линейных стохастических систем второго типа]{%
  Идентификация линейных стохастических систем второго типа
}

\section{Математическая модель задачи идентификации}

Математическая модель задачи идентификации линейной стохастической системы второго типа
является частным случаем общей модели~\eqref{eq:model_general}.
В этом случае скалярно-векторная функция регрессии \( \psi \) носит линейный и
детерминированный характер:
\begin{equation*}
  \psi = c_0 + \sum_j c_j \xi_j.
\end{equation*}

Величины \( \xi_j \) соответствуют наблюдению входов векторной системы,
а коэффициенты \( c_j \) являются параметрами системы, подлежащими оцениванию.

{\color{red}
Следует отметить,
что для упрощения расчетов можно принять идентифицируемую систему скалярной.
}
В этом случае функция регрессии примет еще более простой вид:
\begin{equation}
  \psi = \alpha + \beta \xi,
  \label{eq:fun_linear_scalar}
\end{equation}
где \( \alpha, \beta \) --- постоянная составляющая и коэффициент усиления ---
параметры идентифицируемой линейной системы.

В этом случае модель задачи примет еще более простой вид:
\begin{equation}
  \label{eq:model_linear_scalar}
  \begin{aligned}
  h &= \alpha + \beta \xi, \\
  x &= \xi + \varepsilon_x, \\
  y &= h + \varepsilon_y.
  \end{aligned}
\end{equation}
Здесь, как и в исходной задаче, требуется,
располагая измеренными значениями входа и выхода \( x, y \),
найти оценки \( \hat{\alpha}, \hat{\beta} \) параметров системы \( \alpha, \beta \).


\section{Методы идентификации}

\subsection{Метод наименьших квадратов}

Метод наименьших квадратов ставит своей целью минимизировать сумму вертикальных расстояний
от аппроксимирующей функции (в линейном случае --- гиперплоскости) до результатов наблюдений.
Это соответствует использованию классического критерия идентификации:
\begin{equation*}
  \rho_{\text{к}} = \sum_i \rho_{\text{к}_i} \rightarrow \min.
\end{equation*}

Пусть \( y \) --- вектор-столбец наблюдений выходной переменной \( \eta \),
а \( X \) --- \( (r \times n) \)-матрица измерений вектора \( \xi \)
(\( r \) строк матрицы представляют собой векторы значений входных переменных в данном наблюдении,
а \( n \) столбцов --- векторы значений данной входной переменной во всех наблюдениях).

Алгоритм получения МНК-оценок параметров линейной скалярно-векторной функции регрессии сводится
к вычислению значения вектора~\cite{wiki_lse}
\begin{equation*}
  \hat{\theta} = (X^{T}X)^{-1}X^{T} y.
\end{equation*}

Вычислительная сложность этого алгоритма равна \( O(n^3) \),
при реалистичном условии \( n > r \),
а для обращения матриц используется алгоритм Гаусса-Джордана.

\vspace{2\baselineskip}
\subsection{Метод симметричной аппроксимации}

Метод симметричной аппроксимации минимизирует сумму перпендикулярных расстояний
от аппроксимирующей гиперплоскости до результатов наблюдений.
Это соответствует использованию симметричного критерия идентификации:
\begin{equation*}
  \rho_{\text{с}} = \sum_i \rho_{\text{с}_i} \rightarrow \min.
\end{equation*}

Пусть \( X \) --- \( (m \times n) \)-матрица измерений.
Первые \( r \) и последние \( m - r \) строк этой матрицы представляют собой векторы наблюдений
входных и выходных переменных соответственно,
а столбцы соответствуют векторам \( X_i \) значений переменных в данном наблюдении.

Алгоритм расчета оценок параметров линейной функции регрессии
методом симметричной аппроксимации выглядит следующим образом~\cite{mukha_2016}:
\begin{enumerate}
\item Рассчитываются выборочные моменты случайного вектора наблюдений входных и выходных переменных:
  \begin{equation*}
    \hat{A} = \dfrac{1}{n} \sum_{i=1}^n X_i, \quad
    \hat{D} = \dfrac{1}{n}  \sum_{i=1}^n (X_i - A) (X_i - A)^T.
  \end{equation*}
\item Формируется матрица \( P \), состоящая из собственных векторов матрицы \( \hat{D} \),
  расположенных в порядке убывания соответствующих им собственных чисел.
\item Формируются матрица \( C \), состоящая из \( r \) первых столбцов матрицы
  \( P \). Выполняется разбиение матриц \( X \), \( \hat{A} \) и \( C \) на блоки:
  \begin{equation*}
    C =
    \begin{pmatrix}
      C_r \\
      C_{m-r}
    \end{pmatrix}, \quad
    X =
    \begin{pmatrix}
      X_r \\
      X_{m-r}
    \end{pmatrix}, \quad
    \hat{A} =
    \begin{pmatrix}
      \hat{A}_r \\
      \hat{A}_{m-r}
    \end{pmatrix}.
  \end{equation*}
  Таким образом, матрицы \( C_r \) и \( X_r \) содержат первые \( r \) строк матриц
  \( C \) и \( X \) соответственно,
  а \( C_{m-r} \) и \( X_{m-r} \) --- последние \( m - r \) строк этих матриц.
  Вектор \( \hat{A}_r \) содержит первые \( r \),
  а \( \hat{A}_{m-r} \) --- последние \( m - r \) элементов вектора \( \hat{A} \).
\item Рассчитываются значения оценок параметров зависимости~\eqref{eq:fun_linear_scalar}:
  \begin{equation*}
    \begin{aligned}
      \hat{\beta} &= C_{m-r} (C_r)^{-1}, \\
      \hat{\alpha} &= \hat{A}_{m-r} - \hat{\beta} \hat{A}_r.
    \end{aligned}
  \end{equation*}
\end{enumerate}

Этот алгоритм имеет меньшую вычислительную сложность,
чем МНК: \( O(nm^2) \) против \( O(n^3) \) при \( n > m \).

\section{Численный анализ точности методов идентификации}

Рассматривался объект вида
\begin{equation}
  \label{eq:model}
  \begin{aligned}
  h &= \psi (\alpha, \beta, \xi), \\
  x &= \xi + \varepsilon, \\
  y &= h + \delta
  \end{aligned}
\end{equation}
где
\( \xi, h \) --- фактические значения входной и выходной переменной,
\( \alpha, \beta \) --- фактические значения параметров объекта,
\( \psi \) --- скалярная функция регрессии,
\( x, y \) --- измеренные значения входной и выходной переменной,
\( \varepsilon, \delta \) --- независимые ошибки измерений значений входной и выходной переменной,
распределенные по нормальному закону:
\( \varepsilon = N(0, \sigma_{\varepsilon}), \delta = N(0, \sigma_{\delta}) \).

Исследовалась зависимость точности оценок параметров от с.~к.~о.
ошибок измерений \( \sigma_{\varepsilon}, \sigma_{\delta} \).

В качестве величины, характеризующей точность оценивания,
использовалось среднее Евклидово расстояние в пространстве параметров:
\begin{equation}
  \label{eq:quality}
  d = \frac{1}{k} \sum_{j=1}^k \sqrt{(\hat{\alpha}_j - \alpha)^2 + (\hat{\beta}_j - \beta)^2}.
\end{equation}

В работе~\cite{budny17} было выполнено сравнение точности оценок параметров
\( \hat{\alpha}, \hat{\beta} \) линейной зависимости \( \psi = \alpha + \beta \xi \),
полученных классической линейной регрессией и методом симметричной аппроксимации.

Значения \( \xi_i \) выбирались из равномерного в \( [0, 10] \) распределения.
Для получения каждой оценки \( ( \alpha, \beta ) \) использовались результаты
ста наблюдений \( ( x_i, y_i ), i = \overline{1, n}, n = 100 \).

Расчеты расстояний~\eqref{eq:quality} производились в узлах сетки значений
\( \sigma_{\varepsilon}, \sigma_{\delta} \) в прямоугольнике
\( [0, 2] \times [0, 2] \) с шагом 0{,}1.
В каждом узле сетки вычислялось \( k = 100 \) оценок.


Постановка задачи.
Точность сравнения параметров и точность предсказания.
Условия моделирования.
Графики.
Результаты (таблица)?
Выводы.

% Матричное представление такой модели имеет вид:
% \begin{equation*}
%   y = X \theta + \varepsilon.
% \end{equation*}

% Вектор оценок выходной переменной и вектор остатков регрессии равны
% \( \hat{y} = X \theta, \) и \( e = y - \hat{y} = y - X \theta \) соответственно.
% Сумма квадратов остатков регрессии равна \( RSS = e^T e = (y - X \theta)^T (y - X \theta) \).
% Дифференцируя эту функцию по вектору параметров \( \theta \) и приравняв производные к нулю,
% получим систему уравнений:
% \begin{equation*}
%   (X^T X)b = X^T y.
% \end{equation*}

% Решение этой системы уравнений дает общую формулу МНК-оценок:
% \begin{equation}
%   \hat{\theta} = (X^{T}X)^{-1}X^{T} y.
% \end{equation}

% {\color{red}
%   Пару слов о свойствах и применимости МНК?
%   Ограничения?
% }
